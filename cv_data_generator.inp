/*
   *** CV-DATA-GENERATOR ***

   Package for generating datasets for cross-validation purposes.

   Learning parameters of a prediction function is a major objective. In order to avoid
   over-fitting, the model is trained on a different data sample (the 'training set')
   and evaluated out-of-sample on a so called 'test set'. This approach is known as
   cross-validation (CV).

   This package comprises different approaches of dividing the raw data set into
   a training and test set, respectively, depending on the nature of the underlying data.

   An nice overview can be accessed here:
   https://scikit-learn.org/stable/modules/cross_validation.html

   The separated data sets are stored in array of matrices, which can be read-in for the
   original model training and evaluation.


   written by: Artur Tarassow (atecon@posteo.de; https://github.com/atecon)
*/


function bundle CvDataGenerator (bundle b)

    /*=======================*/
    /* Main package function */
    /*=======================*/

    # Set up the bundle
    bundle self = default_cv_opts()
    if exists(b)
        # override defaults
        self = b + self
    endif

    # Initial checks
    #===============
    check_bundle(&self)

    # Drop missings
    #==============
    # TODO: think about how to handle NAs
    #    smpl --no-missing y X

    # Generate an observation index
    #===============================
    if !inbundle(self, "index") && (typeof(self.X)==7 || typeof(self.X)==2)
        genr index
        self.index = index		# index series for checking that values are related to the right unit
    endif

    # Call evaluation method
    #=======================
    if self.cv_type=="kfold" || self.cv_type=="loo"
        kfold(&self)
    endif

    return self
end function


function bundle default_cv_opts (void)
    
    # Set default values of the bundle
    bundle self = null
    string self.cv_type = "kfold"		# "kfold", "rep_kfold", "loo", LATER: ['recwin', 'rolwin']
    scalar self.n_folds = 5				# divide sample into k groups of samples (default for kfold = 5)
#    scalar self.win_size = 0.25*$nobs	# for 'recwin'/'rolwin': initial window lenght

    return self
end function


function void check_bundle (bundle *self)
    /* Some initial checks for bundle completeness */

    if !inbundle(self, "X")
        funcerr "Provide a list or matrix with data entitled 'X'."
    endif
    if typeof(self.X)==3		# if matrix
        if !inbundle(self, "index")
            funcerr "Provide a vector holding index observations values (via 'genr index') named 'index'."
        elif inbundle(self, "index")
            if typeof(self.index)!=3
                funcerr "Make sure your object 'index' is of type matrix (row vector)."
            endif
        endif
    endif
end function


function bundle gen_xmatrices (bundle *self)
    /* Helper function for generating an
    array for storing the output */    
    matrices self.X_train = array(self.n_folds)
    matrices self.X_test = array(self.n_folds)
    return self
end function


function bundle kfold (bundle *self)
/* Function for running kfold and loo

   Splits data in train/test sets.
   Split dataset into k consecutive folds (without shuffling).
   Each fold can be used once as a validation while the k - 1 remaining
   folds form the training set.
   
   'loo' is a special case of 'kfold' with a single obs. left-out for each fold

   See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html
   */


    self.n_folds = (self.cv_type=="loo") ? $nobs : self.n_folds

    # Rsead-in frame
    X = self.X
    # Index
    if typeof(X)==2 || typeof(X)==7
        series index = self.index
    else
        matrix index  = self.index
    endif
    
    # Generate test/trainig arrays
    gen_xmatrices(&self)

    # Lenght of each test sample
    scalar T = (typeof(X)==7) ? $nobs : rows(X)
    scalar self.foldsize = int( T/self.n_folds )		# lengt of test sample
    scalar rest = T - self.n_folds*self.foldsize		# no. of remainder obs for last iteration

    # Initial test set indices
    scalar start = min(index)
    scalar ende = (start+self.foldsize)-1

    # Start looping
    loop i=1..self.n_folds -q

        # Test set
        if typeof(X)==7								# For list-based approach
            series active_set = 0					# don't drop: indicator for current sample
            smpl index>=start && index<=ende --restrict
            #           smpl X --no-missing
            active_set = 1						# don't drop
            self.X_test[i] = {index} ~ {X}

        else					# For matrix-based approach
            matrix active_set = (index.>=start && index.<=ende)
            self.X_test[i] = selifr(index,active_set) ~ selifr(X,active_set)
        endif

        # Training set
        if typeof(X)==7			# For list-based approach
            smpl active_set==0 --restrict --replace
            #           smpl X --no-missing
            self.X_train[i] = {index} ~ {X}
            smpl full

        else
            active_set = (active_set.=1) ? 0 : 1				# select training set
            self.X_train[i] = selifr(index,active_set) ~ selifr(X,active_set)
        endif

        # update start/ ende
        start += self.foldsize
        ende = start + self.foldsize-1
        ende = (i==self.n_folds) ? (ende+rest) : ende
    endloop

    return self
end function

