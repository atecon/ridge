set verbose off
clear 

#include ridge.gfn --force
include /home/at/git/ridge/ridge.inp		# SET PATH

scalar runEX = 3 		# select an example

# specify some penalty values
# TODO: let's do the following within the function:
/* If the user doesn't specify lambda or nlambda, 
 we set nlambda=100 and lmax=400
 In glmnet() a pre-check is ran: set lmax
 there where most coefficients are 'close to zero'
*/

# Select an example
if runEX==1
    open australia.gdt -q --preserve
    
    # define the variables
    series LHS = ldiff(PAU)
    list RHS = const LHS(-1 to -2) ldiff(PUS) IUS(-1 to -2)  IAU(-1 to -2)
    
elif runEX==2
    # Dataset from glmnet
    open "/home/at/git/ridge/glmnet_QSE.csv" --quiet  --preserve
    #setobs 1 1 --special-time-series
    setobs 1 1 --cross-section

    rename X1 LHS
    list RHS = const dataset
    RHS -= LHS
    
elif runEX==3

    nulldata 1000
    set seed 1234
    setobs 1 1 --time-series
    series e = normal()
    series LHS = 1
    series LHS = 4.3 + 0.8*LHS(-1) - 0.4*LHS(-2) + e
    matrix X = mnormal($nobs,50)
    list RHS = LHS(-1 to -2)
    loop i=1..cols(X) -q
        series S$i = X[,i]
        RHS += S$i
    endloop
    
endif

# Training sample
#scalar train_end = nobs(LHS) - ceil(0.2*(nobs(LHS)))

# Training sample
#smpl 1 train_end

# run the main function with default values
#===========================================
bundle opts = null

# lambda values to evaluate (optional; default 100 values between [0.1,400])
#matrix opts.lambdas = {0.6,5}
# Loss function for CV (optional, default "rmse")
#scalar opts.lmax = 20
string opts.loss_type = "rmse"		# 'me', 'rmse', 'mae', 'mape'

# Run training and evaluation using cross-validation
#====================================================
bundle b_ridge = ridge(LHS, RHS, opts)
print b_ridge

# Plot loss as a function of lambda
plot_loss(&b_ridge)

# Plot solution path
plot_beta(&b_ridge)
stop

# Grab coeff. conditional on optimal lambda
#==========================================
#eval b_ridge.beta_opt

TODOs:
-----
1) Grab beta_opt_min + beta_opt_min_se
2) Grab se_beta_opt_min + se_beta_opt_min_se
3) Print model estimates
4) Clear final bundle
stop




# compare with the (computationally) naive alternative
# (matrix-based function)
matrix rn = ridge_naive(lambdas[1], {LHS}, {RHS})
print rn

## compare with OLS results
print "-- OLS comparison --"
ols LHS RHS

## now test an underdetermined case, #param > #obs:
numk = 20
N = 10
matrix my = mnormal(N, 1)
matrix mX = mnormal(N, numk)

# use the matrix-based backend function directly
eval ridge_svd(lambdas[1], my, mX)[,1]

## do a mechanical test of a no-constant case
RHS = RHS - const
bundle b_ridge = ridge(lambdas, LHS, RHS)
print b_ridge
