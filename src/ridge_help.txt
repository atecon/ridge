Help text for the ridge.gfn package by Sven Schreiber and Artur Tarassow
(please ask questions and report bugs on the gretl mailing list if
possible).

This package implements the ridge regression, a penalized (or shrinkage)
version of linear regression.
Algebraically, the ridge estimator replaces the term (X'X) from OLS by
the term (X'X + lambda * I), where lambda is a scalar penalty parameter
(and I is the identity matrix). The default method in this package to 
calculate the solution is by a singular-value decomposition, and to work
on the standardized variables (mean zero, unit variance). 
For other options see below.

Notice, for a given lambda values you will get different results when
using the well-known glmnet-package for R
(https://cran.r-project.org/web/packages/glmnet/index.html). This is due
to the additional division by the number of observations, N, that GLMNET
uses in their objective function and implicit standardization of Y by
its sample standard deviation. For a comparison with glmnet and the
MASS-package for R see here
<https://github.com/atecon/ridge/blob/master/src/ridge_comparison_with_glmnet_and_mass.inp>.

--------------------
The main function is also called "ridge" and takes the following arguments:

(1) matrix lambdas: required, L-element vector of penalties; passing a 
    scalar is also accepted by gretl
(2) series y: required, the dependent variable
(3) list X: required, the K regressors. If a constant is included, it is 
    moved to the first position.
(4) boolean printout: optional, default is 1 (=yes). Set to 0 to just get 
    the return bundle and not print anything.

Input data is standardized before doing ridge. The output is rescaled to 
match the original data, but since the demeaned data are used, no standard 
errors for the constant term coefficient are currently available.

Returns a bundle with:
- coeffs: KxL matrix of ridge estimators, L being number of lambdas;
    If a constant term was originally included, its coefficient is estimated
    as mean(y - Xtilde'beta), where Xtilde does not contain the constant.
- tstats: KxL matrix of t-statistics; no tstat is available for the constant
    term, so if a constant term was given then tstats[1] is NA.
- vcvs: L-element array of covariance matrices of estimated coefficients,
    each Kbar x Kbar. Kbar is K, or K-1 if a constant term was originally
    included. Each vcv is calculated as s2 * Ri * X'X * Ri, where Ri is the
    inverse of (X'X + lambda * I). (No direct matrix inversion is done,
    though, for numerical reasons.)
- uhats: TxL matrix of residual vectors, one for each lambda. 
- s2s: Lx1 vector of residual variance estimates (uhat'uhat / T), one for
    each lambda.
- num_parameters: 1xL matrix holding the effective number of parameters 
    (corresponds to the trace of the projection matrix, is calculated
    based on the singular values)
- aic: 1xL matrix holding the information criteria computed as:
    aic = (N * log(RSS)) + 		2 * num_parameters
- bic: 1xL matrix holding the information criteria computed as
    bic = (N * log(RSS)) + log(N) * num_parameters
- r2_qcorr: 1xL matrix holding the R-square based on the squared correlation
    between actual and in-sample fitted	values
- r2_qcorr_adjusted: 1xL matrix holding the adjusted version of	'r2_qcorr':
    r2_qcorr_adjusted = 1 - (1-r2_qcorr) * (N-1)/(N-num_parameters-1)
- r2_scores: 1xL matrix holding the coefficient of determination / regression
    score function. The best possible score is 1 and it can be negative
    (because the model can be arbitrarily worse). A constant model that
    always predicts the expected value of y would get a R^2 score of 0.0. 
    This is ported from scikit-learn, for further details see the 'r2_score'
    part in their sklearn/metrics/regression.py module.
- gcv: 1xL matrix holding the generalized cross-validation statistics:
    gcv = mean{ [uhat / (1-num_parameters/N)].^2 }
    For details see Patrick Breheny's slides: 
    <https://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/9-1.pdf>
-------------------

For script use (e.g., in simulations) there are also three matrix-oriented
functions: ridge_svd, ridge_naive, ridge_aux. These functions work on the 
input data as-is, no standardization is done, and they only accept a single
penalty parameter.

ridge_svd...
uses the SVD to calculate results, takes the following arguments:
(1) scalar lambda: required, the penalty parameter.
(2) matrix my: required, data vector of the dependent variable.
(3) matrix mX: required, data matrix of K regressors.
(4) matrix-pointer resids: optional, acts as placeholder to retrieve 
    the residuals (similar to gretl's built-in mols function).
(5) boolean withcov: optional, default 1 (=yes), set to 0 to only 
    return the coeff point estimates.

Returns a matrix:
Per default (withcov==1) a K x (1+K) matrix, the first col is the estimate
beta_ridge, followed by the KxK matrix Cov(beta_ridge). Otherwise just a
Kx1 matrix with the point estimates.

ridge_naive...
uses the textbook formulas directly, mainly for comparison purposes. 
Arguments and return values are identical to ridge_svd.

ridge_aux...
uses an auxiliary OLS regression to produce point estimates only, takes the
following arguments:
(1) scalar lambda: required, the penalty parameter.
(2) matrix my: required, data vector of the dependent variable.
(3) matrix mX: required, data matrix of K regressors.

Returns a Kx1 coefficient matrix.
-------------------

Changelog:
- 1.1, November 2019:
    Refactored code, require gretl 2018a
    Bundle now returns num_parameters, AIC/ BIC information criteria, R-square
    statistics and the generalized cross-validation statistics
- 1.0, September 2017: initial release
