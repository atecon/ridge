## Sample script for the ridge.gfn package for gretl

set verbose off
include ridge.gfn

##### just a tiny helper function needed below ##############
function matrix get_stats (const series y, const series yhat)
   scalar SSR = sum((y - yhat)^2)
   scalar SST = sum((y - mean(y))^2)
   matrix ret = {SSR/$nobs, 1 - SSR/SST}
   cnameset(ret, "MSE R^2")
   return ret
end function
#############################################################

open australia.gdt --quiet

# specify some penalty values
matrix lambdas = {0.36, 0.5, 1.5, 2.5, 5.0, 10, 25}

# define the variables
series LHS = ldiff(PAU)
list RHS = const ldiff(PUS) IUS IAU(0 to -4)

smpl --no-missing LHS RHS

# Now we'll compare OLS forecasts with a series of 
# predictive ridge regressions, using the specified
# penalties (lambdas).
# It turns out that the penalized ridge can be better 
# out of sample than OLS.
#
# (Never mind the negative R2s which are relative to 
# the out-of-sample mean value, a statistic which is 
# not available at the time of the forecast. Only the 
# ranking of the R2s matters.)

# reserve some out-of-sample observations
smpl ; -13
smpl

ols LHS RHS --simple-print
matrix bols = $coeff

print "#### Run the main ridge function with default values ####"
print "#### (and several penalties)                         ####"
bundle b_ridge = ridge(lambdas, LHS, RHS)

smpl 1988:1 $tmax
printf "Out of sample comparison, %d observations\n", $nobs
printf "First OLS:\n\n"
eval get_stats(LHS, lincomb(RHS, bols))

matrix R = {}
loop j=1..nelem(lambdas) -q
   R |= get_stats(LHS, lincomb(RHS, b_ridge.coeffs[,j]))
endloop
R = lambdas' ~ R
cnameset(R, "lambda MSE R^2")
printf "Now Ridge:\n\n"
print R

##############################################
printf " ------- ( End of forecasting exercise ) ----------\n\n"

print " -------------------- Showing some internals -------------------"

smpl --full
smpl --no-missing LHS RHS # revert sample
print "-- compare with the (computationally naive) alternative"
print "  (this function works on matrices)"
matrix rn = ridge_naive(lambdas[1], {LHS}, {RHS})
print rn

print "-- now test an underdetermined case, #param > #obs:"
numk = 20
N = 10
matrix my = mnormal(N, 1)
matrix mX = mnormal(N, numk)

print " ... using the matrix-based backend function ridge_svd directly"
eval ridge_svd(lambdas[1], my, mX)[,1]

print "-- do a mechanical test of a no-constant case"
RHS = RHS - const
bundle b_ridge = ridge(lambdas[1:6], LHS, RHS)

print " ... the result bundle:"
print b_ridge

print " ... the collected coefficient 'paths':"
matrix coeffs = b_ridge.coeffs
coeffs

print "-- show some of the statistics and indicators"
# no. of parameters
matrix num_parameters = b_ridge.num_parameters	
num_parameters

# r2 as computed in scikit-learn
matrix r2_scores = b_ridge.r2_scores			
r2_scores

# squared correlation between realizations 'y' and fitted values 'yhat'
matrix r2_qcorr = b_ridge.r2_qcorr				
r2_qcorr

# squared correlation between realizations 'y' and fitted values 'yhat'
matrix r2_qcorr_adjusted = b_ridge.r2_qcorr_adjusted	
r2_qcorr_adjusted

matrix aic = b_ridge.aic
aic
matrix bic = b_ridge.bic
bic

matrix gcv = b_ridge.gcv
gcv
