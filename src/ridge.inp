function bundle ridge (matrix lambdas "Penalty parameters",
                       series y "Dependent variable",
                       const list X "List of regressors",
                       bool printout[1] "Print results?")
    /* Input data is standardized before doing ridge.
       The output is rescaled to match the original data.
       Results for a constant (if present) are ordered first.
       A scalar input for lambdas is treated by gretl as 1x1 matrix.
       Returns a bundle with:
       coeffs: KxL matrix of ridge estimators, L being number of lambdas;
       If a constant term was originally included, its coefficient
       is estimated as mean(y - Xtilde'beta), where Xtilde does not
       contain the constant.
       tstats: KxL matrix of t-statistics; no tstat is available for
       the constant term, so if a constant term was given then
       tstats[1] is NA.
       vcvs: L-element array of matrices, each Kbar x Kbar.
       Kbar is K, or K-1 if a constant term was originally included.
       uhats: TxL matrix of residual vectors, one for each lambda
       s2s: Lx1 vector of residual variance estimates (res'res / T)
       FIXME: R2 ?
    */

    bundle self = null
    self.lambdas = vec(lambdas)
    self.n_lambdas = nelem(self.lambdas)

    if $datatype==1
        smpl --no-missing y X
    elif $datatype==2
        smpl --contiguous y X
    endif

    series self.y = y

    # check for constant
    list self.Xtilde = X - const
    self.wconst = nelem(self.Xtilde) < nelem(X) ? 1 : 0

    # standardize and save factors for later
    stdize_y_and_x(&self)

    ## cycle through the various lambdas
    initialize_storage_matrices(&self)
    loop_through_lambda_values(&self)

    get_xreorder_list(&self, X)

    set_rowname_betas_and_tstats(&self)

    print_ridge_results(&self, printout)

    # attached column names to matrices
    add_column_names_matrices(&self)
    
    # copy to output
    return defbundle("coeffs", self.betas, "tstats", self.tstats, "vcvs", self.betaCovs, \
      "uhats", self.resids, "s2s", self.s2s, "r2_scores", self.r2_scores, \
      "r2_qcorr", self.r2_qcorr, "r2_qcorr_adjusted", self.r2_qcorr_adj, \
      "aic", self.aic, "bic", self.bic, "gcv", self.gcv, "num_parameters", self.num_parameters)

end function

function void add_column_names_matrices (bundle *self)
    /* Add lambda values as string values to name columns of matrices */

    strings str_lambdas = array(self.n_lambdas)
    loop i=1..self.n_lambdas -q
        str_lambdas[i] = sprintf("Î»=%.4f", self.lambdas[i])
    endloop

    strings to_be_named = defarray("betas", "tstats", "resids", "s2s", "r2_scores", \
      "r2_qcorr", "r2_qcorr_adj", "aic", "bic", "gcv", "num_parameters")
      
    loop i=1..nelem(to_be_named) -q
        string item = sprintf("self.%s", to_be_named[i])
        cnameset(@item, str_lambdas)
    endloop
end function


function void set_rowname_betas_and_tstats (bundle *self)
    rnameset(self.betas, varnames(self.Xreorder))
    rnameset(self.tstats, varnames(self.Xreorder))
end function


function void get_xreorder_list (bundle *self, const list X)
    if self.wconst
        list self.Xreorder = const self.Xtilde
    else
        list self.Xreorder = X
    endif
end function


function void loop_through_lambda_values (bundle *self)
/* Helper function estimating ridge + retrieving various stats
    for different shrinkage lambda parameters */

    scalar idx = 1+self.wconst
    loop i=1..self.n_lambdas -q

        matrix singular_value = {}
        matrix temp = ridge_svd(self.lambdas[i], self.my, self.mX, , &singular_value)

        self.betas[idx:,i] = temp[,1]
        self.betaCovs[i] = temp[, 2:]
        self.tstats[idx:,i] = self.betas[idx:,i] ./ sqrt(diag(temp[,2:]))

        # rescale
        self.betas[idx:,i] = self.betas[idx:,i] .* (self.ysd ./ self.Xsd')
        self.betaCovs[i] = self.betaCovs[i] * self.ysd^2 ./ (self.Xsd'self.Xsd)

        # construct residuals -- may have non-zero mean
        self.resids[,i] = get_residuals({self.y}, {self.Xtilde}, \
          self.betas[idx:,i]) # original data (w/o const)

        # estimate residual variance
        self.s2s[i] = get_residuals_variance(cdemean(self.resids[,i]), self.my)

        # insert the constant term
        self.betas[1,i] = (self.wconst) ? meanc(self.resids[,i]) : self.betas[1,i]
        self.tstats[1,i] = (self.wconst) ? NA : self.tstats[1,i]

        # get number of ridge parameters
        self.num_parameters[i] = effective_num_of_params(singular_value, self.lambdas[i])

        matrix aic_bic = ridge_aic_bic(self.num_parameters[i], sumc(self.resids[,i].^2), rows(self.my))
        self.aic[i] = aic_bic[1]
        self.bic[i] = aic_bic[2]
        self.gcv[i] = generalized_cv_stats(self.resids[,i], rows(self.my), self.num_parameters[i])

        # Compute r-square
        matrix r_squares = r2_stats({self.y}, get_y_pred({self.Xtilde}, self.betas[1+self.wconst:,i]), self.num_parameters[i])
        self.r2_qcorr[i] = r_squares[1]
        self.r2_qcorr_adj[i] = r_squares[2]
        self.r2_scores[i] = r_squares[3]
        
    endloop
end function


function void print_ridge_results (bundle *self, const bool printout)
    strings lbl = varnames(self.Xreorder) + defarray("Penalty", "Resid SD", "No. of parameters", \
      "R^2", "R^2 (adj)", "R^2 score", "AIC", "BIC", "G-CV")

    print "Ridge regression results"
    loop i = 1..self.n_lambdas -q
        matrix cmat = self.betas[,i] ~ (self.betas[,i] ./ self.tstats[,i])
        matrix extra = self.lambdas[i] | sqrt(self.s2s[i]) | self.num_parameters[i] | self.r2_qcorr[i] | \
        self.r2_qcorr_adj[i] | self.r2_scores[i] | self.aic[i] | self.bic[i] | self.gcv[i]
        modprint cmat lbl extra
    endloop
end function


function void stdize_y_and_x (bundle *self)
    /* Demeaning series y and list X and compute associated std.-errors. */
    matrix my = cdemean({self.y})
    scalar self.ysd = sdc(my)
    self.my = my ./ self.ysd
    matrix mX = cdemean({self.Xtilde})
    matrix self.Xsd = sdc(mX)			# gives row vector
    self.mX = mX ./ self.Xsd
end function

function void initialize_storage_matrices (bundle *self)
    scalar k = (self.wconst==1) ? (nelem(self.Xtilde)+1) : nelem(self.Xtilde)
    matrix self.betas = NA*zeros(k, self.n_lambdas)
    matrix self.tstats = NA*zeros(k, self.n_lambdas)
    matrix self.resids = NA*zeros(rows(self.my), self.n_lambdas)
    matrix self.s2s = zeros(self.n_lambdas, 1)
    matrices self.betaCovs = array(self.n_lambdas)
    
    matrix self.num_parameters = NA*zeros(1, self.n_lambdas)
    matrix self.aic = NA*zeros(1, self.n_lambdas)
    matrix self.bic = NA*zeros(1, self.n_lambdas)
    matrix self.r2_scores = NA*zeros(1, self.n_lambdas)
    matrix self.r2_qcorr = NA*zeros(1, self.n_lambdas)
    matrix self.r2_qcorr_adj = NA*zeros(1, self.n_lambdas)
    matrix self.gcv = NA*zeros(1, self.n_lambdas)
end function


function matrix ridge_svd (const scalar lambda,
                           const matrix my,
                           const matrix mX,
                           matrix *resids[null],
                           matrix *sv[null] "Singular values",
                           bool withcov[1])
    # Ridge function using SVD
    # my: Tx1 vector
    # mX: TxK design matrix
    matrix out, U, Vt	# Vt: V transposed
    matrix sv = svd(mX, &U, &Vt)
    matrix E_lda_row = 1 / (sv.^2 + lambda)
    if withcov	# we will need the ridge inverse later
        # ridge inverse
        matrix ridgeI = (Vt' .* E_lda_row) * Vt
        # beta, using this inverse
        out = ridgeI * mX'my
        matrix res = get_residuals(my, mX, out)
        scalar s2 = get_residuals_variance(res, mX)
        matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
        out ~= covbeta
    else
        # beta, using the SVD output differently
        out = ( Vt' .* (sv .* E_lda_row) ) * U'my
    endif
    if exists(resids)
        resids = withcov ? res : my - mX*out
    endif
    return out
end function

function matrix ridge_naive (const scalar lambda,
                             const matrix my,
                             const matrix mX,
                             matrix *resids[null],
                             bool withcov[1])
    # Ridge function using matrix algebraic formulas directly
    # Returns a K x (1 + K) matrix if withcov==1,
    # the first col is the estimate beta_ridge,
    # followed by the KxK matrix Cov(beta_ridge).
    # Otherwise just a Kx1 matrix.
    # If 'resids' is provided, it will be filled with residuals.
    matrix ridgeI = inv(mX'mX + lambda * I(cols(mX)))
    # beta
    matrix out = ridgeI * mX'my
    if exists(resids) || withcov
        matrix res = get_residuals(my, mX, out)
        if withcov
            scalar s2 = get_residuals_variance(res, mX)
            matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
            out ~= covbeta
        endif
        if exists(resids)
            resids = res
        endif
    endif
    return out
end function

function matrix ridge_aux (const scalar lambda,
                           const matrix my,
                           const matrix mX)
    # Ridge function using auxiliary OLS
    # Here we do not obtain the Cov(b) matrix,
    # return is just Kx1 beta vector.
    K = cols(mX)
    matrix mXa = mX | (sqrt(lambda) * I(K))
    matrix mya = my | zeros(K,1)
    matrix beta = mols(mya, mXa)
    return beta
end function

function matrix get_y_pred (const matrix mX, const matrix bhat)
    /* Helper function for computing fitted values*/
    return mX*bhat
end function


function matrix get_residuals (const matrix my, const matrix mX, const matrix bhat)
    /* Helper function for computing residuals */
    return my - get_y_pred(mX, bhat)
end function

function scalar get_residuals_variance (const matrix resids, const matrix mX)
    /* Helper function for computing the residuals' variance */
    return resids'resids / rows(mX)
end function


function scalar effective_num_of_params (const matrix singular_value, const scalar lambda)
    /* Compute the effective number of parameters based on squared
    singular_values and the number of ridge parameters. This should be equal to
    tr(H). */
    
    singular_value_sq = vec(singular_value.^2)    
    return sumc( singular_value_sq ./ (singular_value_sq + lambda) )
end function


function matrix ridge_aic_bic (const scalar num_param "Number of parameters",
                               const scalar RSS "Residual sum of squares",
                               const int N "No. of observations")
    /* Compute both AIC and BIC criteria for ridge regression */    
    scalar s = N*log(RSS)
    scalar aic = s+2*num_param
    scalar bic = s+num_param*log(N)
    return aic | bic
end function


function matrix r2_stats (matrix y_true "T by 1 vector of realizations",
                          matrix y_pred "T by 1 vector of estimated values",
                          const scalar num_parameters "No. of parameters")
    /* Helper function for computing different R-square statistics. */

    matrix y_true = vec(y_true)
    matrix y_pred = vec(y_pred)

    if rows(y_true) != rows(y_pred)
        printf "\nError: Vectors y_true and y_pred are of different length.\n"
        return NA
    endif
    if rows(y_pred)<2
        printf "\nError: R^2 score is not well-defined with less than two samples.\n"
        return NA
    endif
    if isconst(y_true)
        printf "\nError: Vector y_true is constant.\n"
        return NA
    endif

    matrix R = NA*zeros(3,1)
    R[1] = r2_qcorr(&y_true, &y_pred)
    R[2] = r2_qcorr_adjusted(R[1], rows(y_true), num_parameters)
    R[3] = r2_score(&y_true, &y_pred)
    rnameset(R, "r2_qcorr r2_qcorr_adj r2_score")

    return R
end function


function scalar r2_qcorr (const matrix *y_true,
                              const matrix *y_pred)
    /* R-square based on quadratic correlation */
    return corr(y_true, y_pred)^2
end function


function scalar r2_qcorr_adjusted (const scalar r2 "R-square from r2_qcorr()",
  const int n "Sample length",
  const scalar num_parameters "No. of ridge parameters used")
    /* Adjusted R-square based on quadratic correlation */
    
    scalar nominator = n-1
    scalar denominator = n-num_parameters-1
    
    return 1 - (1-r2) * nominator/denominator
end function


function scalar r2_score (const matrix *y_true,
                          const matrix *y_pred)
    /* R^2 (coefficient of determination) regression score function.
       Best possible score is 1.0 and it can be negative (because the
       model can be arbitrarily worse). A constant model that always
       predicts the expected value of y, disregarding the input features,
       would get a R^2 score of 0.0.

       Notes
       -----
       This is not a symmetric function.
       Unlike most other scores, R^2 score may be negative (it need not actually
       be the square of a quantity R).
       This metric is not well-defined for single samples and will return a NaN
       value if n_samples is less than two.
       References
       ----------
       <https://github.com/scikit-learn/scikit-learn/blob/1495f69242646d239d89a5713982946b8ffcf9d9/sklearn/metrics/regression.py#L449>

       `Wikipedia entry on the Coefficient of determination
       <https://en.wikipedia.org/wiki/Coefficient_of_determination>`
    */

    # Initialize check values
    scalar nonzero_denominator = 0
    scalar nonzero_numerator = 0
    scalar valid_score = 0

    # Compute statistics
    scalar numerator = sumc( (y_true-y_pred).^2)
    scalar denominator = sumc( (y_true-mean(y_true)).^2 )

    scalar nonzero_denominator = (denominator!=0) ? 1 : nonzero_denominator
    scalar nonzero_numerator = (numerator!=0) ? 1 : nonzero_numerator

    scalar valid_score = (nonzero_denominator && nonzero_numerator) ? 1 : valid_score

    scalar output_scores = 1

    return (valid_score) ? (1-numerator/denominator) : output_scores
end function

function scalar generalized_cv_stats (const matrix resids,
                                const int N[1::] "Sample length",
                                const scalar effective_num_of_params)
    /* Compute generalized cross-validation statistics. */

    return meanc( (resids / (1 - effective_num_of_num_params/N)).^2 )
end function





