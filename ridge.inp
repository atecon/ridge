function bundle default_ridge_opts ()

    # Set default values
    bundle self = null
    scalar self.nlambda = 100		# no. of shrinkage params to check
    scalar self.lmax = 400			# maximal shrinkage param value
    matrix self.lambdas = vec(seq(0.1, self.lmax, self.lmax/self.nlambda))

    # For gridsearch
    # TODO: use also in-sample based information criteria
    string self.loss_type = "rmse"		# Loss function: 'rmse', 'mae', 'mape'"
    if self.loss_type == "rmse"
        scalar self.sel_loss = 2
    elif self.loss_type == "mae"
        scalar self.sel_loss = 3
    elif self.loss_type == "mape"
        scalar self.sel_loss = 5
    endif    
    # ???
    string self.cv_type = ""			# Type of cross-validation"

    scalar self.printout = 1		# Print results (default: True)

    return self
end function


function bundle ridge (series y "Dep. variable",
                       const list X "Regressors",
                       bundle opts[null])

    /* Input data is standardized before doing ridge.
       The output is rescaled to match the original data.
       Results for a constant (if present) are ordered first.
       A scalar input for lambdas is treated by gretl as 1x1 matrix.
       Returns a bundle with:
       coeffs: KxL matrix of ridge estimators, L being number of lambdas;
       If a constant term was originally included, its coefficient
       is estimated as mean(y - Xtilde'beta), where Xtilde does not
       contain the constant.
       tstats: KxL matrix of t-statistics; no tstat is available for
       the constant term, so if a constant term was given then
       tstats[1] is NA.
       vcvs: L-element array of matrices, each Kbar x Kbar.
       Kbar is K, or K-1 if a constant term was originally included.
       uhats: TxL matrix of residual vectors, one for each lambda
       s2s: Lx1 vector of residual variance estimates (res'res / T)
       FIXME: R2 ?
    */

    /*=======================*/
    /* Main package function */
    /*=======================*/
    
    # Set up the bundle
    bundle self = default_ridge_opts()
    if exists(opts)
        # override defaults
        self = opts + self
        self.nlambda = rows(vec(self.lambdas))
    endif

    # Drop missings
    smpl --no-missing y X
    
    # Put some stuff to the bundle
    #=============================
    series self.y = y
    list self.X = X
    list self.Xtilde = X - const    
    # check for constant
    scalar self.wconst = nelem(self.Xtilde) < nelem(self.X) ? 1 : 0

    # standardize and save factors for later
    matrix my = cdemean({y})
    scalar self.ysd = sdc(my)
    self.my = my ./ self.ysd
    matrix mX = cdemean({self.Xtilde})
    matrix self.Xsd = sdc(mX)			# gives row vector
    self.mX = mX ./ self.Xsd

    ## cycle through the various lambdas
    est_ridge(&self)
    
    # insert the constant term
    if self.wconst
        self.betas = meanc(self.resids) | self.betas
        self.tstats = NA | self.tstats
        list Xreorder = const self.Xtilde
    else
        list Xreorder = self.X
    endif
    
    # labels
    strings self.Xnames = varnames(Xreorder)
    
    rnameset(self.betas, self.Xnames)
    rnameset(self.tstats, self.Xnames)
    /*
    if self.printout
        strings lbl = self.Xnames + defarray("Penalty", "Resid SD")
        print "Ridge regression results"
        loop i=1..self.nlambda -q
            matrix cmat = self.betas[,i] ~ (self.betas[,i] ./ self.tstats[,i])
            matrix extra = self.lambdas[i] | sqrt(self.s2s[i])
            modprint cmat lbl extra
        endloop
    endif
    */

    # copy to output
    /*
    return defbundle("lambdas", self.lambdas, "coeffs", self.betas, \
      "tstats", self.tstats, "vcvs", self.betaCovs, "uhats", self.resids, \
      "s2s", self.s2s, "Xnames", self.Xnames)      
    */
    return self
end function


function bundle ridge_gridsearch (bundle *self "Created bundle from calling ridge() before")

    /* This function is supposed to run a grid-search and select the
    optimal lambda-value for some loss function. Most likely you want
    to use for y and X observations from the test set. */

    #smpl --no-missing y X

    # store loss values
    matrix loss = zeros(self.nlambda,1)

    # Compute forecast evaluation statistics for each eta value
    # Predict and evaluate
    loop i=1..self.nlambda -q
        matrix fc = {self.X} * self.betas[,i]		# construct forecast
        loss[i] = fcstats(self.y, fc)[self.sel_loss]
        #cnam[i] = sprintf("lambda=%g", b.lambdas[i])
    endloop
    #rnameset(FCstat, cnam)
    scalar pos_lambda = iminc(loss)					# position of lambda minimizing loss
    scalar self.lambda_opt = self.lambdas[pos_lambda]	# optimal lambda value
    scalar self.loss_opt = loss[pos_lambda]
    matrix self.loss = loss
    matrix self.beta_opt = self.betas[,pos_lambda]		# optimal coefficient vector
    rnameset(self.beta_opt, self.Xnames)

#    string self.loss_type = loss_type
    if self.printout
        printf "\n*** '%s' minimized for lambda = %.2f ***\n", self.loss_type, self.lambda_opt
        printf "*** Loss ('%s') is = %.5f ***\n", self.loss_type, self.loss_opt
    endif
    
    return self
end function


function bundle est_ridge (bundle *self)

    /* Actual computation of ridge and related stats */
    
    matrix res = zeros($nobs,1)
    matrix temp = {}
    
    # Matrix holder
    matrix self.betas = zeros(nelem(self.Xtilde), self.nlambda)
    matrix self.tstats = zeros(nelem(self.Xtilde), self.nlambda)
    matrix self.resids = zeros($nobs, self.nlambda)
    matrix self.s2s = zeros(self.nlambda, 1)
    matrices self.betaCovs = array(self.nlambda)

    loop i = 1..self.nlambda -q
        matrix temp = ridge_svd(self.lambdas[i], self.my, self.mX)
        self.betas[,i] = temp[,1]
        self.betaCovs[i] = temp[, 2:]
        self.tstats[,i] = self.betas[,i] ./ sqrt(diag(temp[, 2:]))
        
        # rescale
        self.betas[,i] = self.betas[,i] .* (self.ysd ./ self.Xsd')
        self.betaCovs[i] = self.betaCovs[i] * self.ysd^2 ./ (self.Xsd'self.Xsd)
        
        # construct residuals
        matrix res = {self.y} - {self.Xtilde} * self.betas[,i] # original data (w/o const)
        self.resids[,i] = res				# resids may have non-zero mean
        
        # estimate residual variance
        res = cdemean(res)
        self.s2s[i] = res'res / rows(self.my)
    endloop

    return self
end function



function void plot_loss (bundle *self,
                         string path "Set path or 'display'")
    # Plot the loss function for each iteration
    # TODO: glmnet also computes CIs based on CV-results
    
    matrix mat = ( vec(self.loss) ~ log(vec(self.lambdas)) )
    scalar ymx = maxc(mat[,1])*1.02
    plot mat
        options with-lines fit=none
        printf "set ylabel '%s' font ', 12'", self.loss_type
        printf "set xlabel 'log(lambda)' font ', 12'"
        literal set linetype 1 lc rgb "red" lw 2
        literal set linetype 2 lc rgb "black" lw 2 pt 3
        printf "set yrange[:%g]", ymx
        printf "set key outside right"
        printf "set arrow from %g,graph(0,0) to %g,graph(1,1) \
        nohead lc rgb 'blue' lw 1.5", log(self.lambda_opt), log(self.lambda_opt)
    end plot --output="@path"
end function


function void plot_beta (bundle *self,
                         string path "Set path or 'display'")
    # Plot the solution path as a function of log(lambda)
    matrix mat = self.betas' ~ log(self.lambdas)
    plot mat
        options with-lines fit=none
        printf "set ylabel 'Coefficients' font ', 12'"
        printf "set xlabel 'log(lambda)' font ', 12'"
        printf "set arrow from %g,graph(0,0) to %g,graph(1,1) \
          nohead lc rgb 'blue' lw 1.5", log(self.lambda_opt), log(self.lambda_opt)
        printf "set key outside below"
        literal set grid
    end plot --output="@path"
end function




function matrix ridge_svd (scalar lambda,
                           const matrix my,
                           const matrix mX,
                           matrix *resids[null],
                           bool withcov[1])
    # Ridge function using SVD
    # my: Tx1 vector
    # mX: TxK design matrix
    matrix out, U, Vt	# Vt: V transposed
    matrix sv = svd(mX, &U, &Vt)
    matrix E_lda_row = 1 / (sv.^2 + lambda)
    if withcov	# we will need the ridge inverse later
        # ridge inverse
        matrix ridgeI = (Vt' .* E_lda_row) * Vt
        # beta, using this inverse
        out = ridgeI * mX'my
        matrix res = my - mX*out
        scalar s2 = res'res / rows(mX)
        matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
        out ~= covbeta
    else
        # beta, using the SVD output differently
        out = ( Vt' .* (sv .* E_lda_row) ) * U'my
    endif
    if exists(resids)
        resids = withcov ? res : my - mX*out
    endif
    return out
end function

function matrix ridge_naive (scalar lambda,
                             const matrix my,
                             const matrix mX,
                             matrix *resids[null],
                             bool withcov[1])
    # Ridge function using matrix algebraic formulas directly
    # Returns a K x (1 + K) matrix if withcov==1,
    # the first col is the estimate beta_ridge,
    # followed by the KxK matrix Cov(beta_ridge).
    # Otherwise just a Kx1 matrix.
    # If 'resids' is provided, it will be filled with residuals.
    matrix ridgeI = inv(mX'mX + lambda * I(cols(mX)))
    # beta
    matrix out = ridgeI * mX'my
    if exists(resids) || withcov
        matrix res = my - mX*out # y - Xb
        if withcov
            scalar s2 = res'res / rows(mX)
            matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
            out ~= covbeta
        endif
        if exists(resids)
            resids = res
        endif
    endif
    return out
end function

function matrix ridge_aux (scalar lambda,
                           const matrix my,
                           const matrix mX)
    # Ridge function using auxiliary OLS
    # Here we do not obtain the Cov(b) matrix,
    # return is just Kx1 beta vector.
    K = cols(mX)
    matrix mXa = mX | (sqrt(lambda) * I(K))
    matrix mya = my | zeros(K,1)
    matrix beta = mols(mya, mXa)
    return beta
end function
