function bundle ridge (matrix lambdas "Penalty params",
                       series y "Dep. variable",
                       const list X "Regressors",
                       bool printout[1] "Print results?")
    /* Input data is standardized before doing ridge.
       The output is rescaled to match the original data.
       Results for a constant (if present) are ordered first.
       A scalar input for lambdas is treated by gretl as 1x1 matrix.
       Returns a bundle with:
       coeffs: KxL matrix of ridge estimators, L being number of lambdas;
       If a constant term was originally included, its coefficient
       is estimated as mean(y - Xtilde'beta), where Xtilde does not
       contain the constant.
       tstats: KxL matrix of t-statistics; no tstat is available for
       the constant term, so if a constant term was given then
       tstats[1] is NA.
       vcvs: L-element array of matrices, each Kbar x Kbar.
       Kbar is K, or K-1 if a constant term was originally included.
       uhats: TxL matrix of residual vectors, one for each lambda
       s2s: Lx1 vector of residual variance estimates (res'res / T)
       FIXME: R2 ?
    */
    lambdas = vec(lambdas)	# ensure 1-dim
    L = rows(lambdas)
    smpl --no-missing y X
    # check for constant
    list Xtilde = X - const
    wconst = nelem(Xtilde) < nelem(X) ? 1 : 0
    # standardize and save factors for later
    matrix my = cdemean({y})
    scalar ysd = sdc(my)
    my = my ./ ysd
    matrix mX = cdemean({Xtilde})
    matrix Xsd = sdc(mX)	# gives row vector
    mX = mX ./ Xsd
    ## cycle through the various lambdas
    matrix s2s, res, temp
    matrix betas = zeros(nelem(Xtilde), L)
    matrix tstats = zeros(nelem(Xtilde), L)
    matrix resids = zeros($nobs, L)
    matrix s2s = zeros(L,1)
    matrices betaCovs = array(L)
    loop i = 1..L -q
        matrix temp = ridge_svd(lambdas[i], my, mX)
        betas[,i] = temp[,1]
        betaCovs[i] = temp[, 2:]
        tstats[,i] = betas[,i] ./ sqrt(diag(temp[, 2:]))
        # rescale
        betas[,i] = betas[,i] .* (ysd ./ Xsd')
        betaCovs[i] = betaCovs[i] * ysd^2 ./ (Xsd'Xsd)
        # construct residuals
        matrix res = {y} - {Xtilde} * betas[,i] # original data (w/o const)
        resids[,i] = res	# resids may have non-zero mean
        # estimate residual variance
        res = cdemean(res)
        s2s[i] = res'res / rows(my)
    endloop
    # insert the constant term
    if wconst
        betas = meanc(resids) | betas
        tstats = NA | tstats
        list Xreorder = const Xtilde
    else
        list Xreorder = X
    endif
    # labels
    strings Xnames = varnames(Xreorder)
    rnameset(betas, Xnames)
    rnameset(tstats, Xnames)
    if printout
        strings lbl = Xnames + defarray("Penalty", "Resid SD")
        print "Ridge regression results"
        loop i = 1..L -q
            matrix cmat = betas[,i] ~ (betas[,i] ./ tstats[,i])
            matrix extra = lambdas[i] | sqrt(s2s[i])
            modprint cmat lbl extra
        endloop
    endif
    # copy to output
    return defbundle("lambdas", lambdas, "coeffs", betas, "tstats", tstats, "vcvs", betaCovs, "uhats", resids, "s2s", s2s, \
      "Xnames", Xnames)
end function

function void ridge_gridsearch ( bundle *b "Created bundle from calling ridge() before",
      series y "Dep. variable",
      const list X "Regressors",
      string loss_type "Loss function: 'rmse', 'mae', 'mape'",
      string cv_type "Type of cross-validation",
      bool printout[1] "Print results?")

    # This function is supposed to run a grid-search and select the optimal lambda-value
    # for some loss function.
    # Most likely you want to use for y and X observations from the test set.

    smpl --no-missing y X

    L = rows(b.lambdas)
    matrix loss = zeros(L,1)
    #strings cnam = array(L)

    # Compute forecast evaluation statistics for each eta value
    if loss_type == "rmse"		# 'rmse', 'mae', 'mape'
        scalar sel = 2
    elif loss_type == "mae"
        scalar sel = 3
    elif loss_type == "mape"
        scalar sel = 5
    endif
    # Predict and evaluate
    loop i=1..L -q
        matrix fc = {X} * b.coeffs[,i]		# construct forecast
        loss[i] = fcstats(y, fc)[sel]
        #cnam[i] = sprintf("lambda=%g", b.lambdas[i])
    endloop
    #rnameset(FCstat, cnam)
    scalar pos_lambda = iminc(loss)			# position of lambda minimizing loss
    scalar b.lambda_opt = b.lambdas[pos_lambda]		# optimal lambda value
    matrix b.coef_opt = b.coeffs[,pos_lambda]		# optimal coefficient vector
    rnameset(b.coef_opt, b.Xnames)
    scalar b.loss_opt = loss[pos_lambda]
    matrix b.loss = loss
    string b.loss_type = loss_type
    if printout
        printf "\n*** '%s' minimized for lambda = %.2f ***\n", loss_type, b.lambda_opt
        printf "*** Loss ('%s') is = %.5f ***\n", loss_type, b.loss_opt
    endif
end function

function void Loss_Plot (bundle *b,
                         string path "Set path or 'display'")
    # Plot the loss function for each iteration
    # TODO: glmnet also computes CIs based on CV-results

    matrix mat = b.loss ~ log(b.lambdas)
    scalar ymx = max(mat[,1])*1.02
    plot mat
        options with-lines fit=none
        printf "set ylabel '%s' font ', 14'", b.loss_type
        printf "set xlabel 'log(lambda)' font ', 14'"
        literal set linetype 1 lc rgb "red" lw 2
        literal set linetype 2 lc rgb "black" lw 2 pt 3
        printf "set yrange[:%g]", ymx
        printf "set arrow from 0,graph(%g,%g) to 0,graph(0,0) nohead ", b.lambda_opt, b.lambda_opt
    end plot --output="@path"
end function


function void Coef_Plot (bundle *b,
                         string path "Set path or 'display'")
    # Plot the solution path as a function of log(lambda)
    matrix mat = b.coeffs' ~ log(b.lambdas)    
    plot mat
        options with-lines fit=none
        printf "set ylabel 'Coefficients' font ', 14'"
        printf "set xlabel 'log(lambda)' font ', 14'"
        literal set grid
    end plot --output="@path"
end function




function matrix ridge_svd (scalar lambda,
                           const matrix my,
                           const matrix mX,
                           matrix *resids[null],
                           bool withcov[1])
    # Ridge function using SVD
    # my: Tx1 vector
    # mX: TxK design matrix
    matrix out, U, Vt	# Vt: V transposed
    matrix sv = svd(mX, &U, &Vt)
    matrix E_lda_row = 1 / (sv.^2 + lambda)
    if withcov	# we will need the ridge inverse later
        # ridge inverse
        matrix ridgeI = (Vt' .* E_lda_row) * Vt
        # beta, using this inverse
        out = ridgeI * mX'my
        matrix res = my - mX*out
        scalar s2 = res'res / rows(mX)
        matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
        out ~= covbeta
    else
        # beta, using the SVD output differently
        out = ( Vt' .* (sv .* E_lda_row) ) * U'my
    endif
    if exists(resids)
        resids = withcov ? res : my - mX*out
    endif
    return out
end function

function matrix ridge_naive (scalar lambda,
                             const matrix my,
                             const matrix mX,
                             matrix *resids[null],
                             bool withcov[1])
    # Ridge function using matrix algebraic formulas directly
    # Returns a K x (1 + K) matrix if withcov==1,
    # the first col is the estimate beta_ridge,
    # followed by the KxK matrix Cov(beta_ridge).
    # Otherwise just a Kx1 matrix.
    # If 'resids' is provided, it will be filled with residuals.
    matrix ridgeI = inv(mX'mX + lambda * I(cols(mX)))
    # beta
    matrix out = ridgeI * mX'my
    if exists(resids) || withcov
        matrix res = my - mX*out # y - Xb
        if withcov
            scalar s2 = res'res / rows(mX)
            matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
            out ~= covbeta
        endif
        if exists(resids)
            resids = res
        endif
    endif
    return out
end function

function matrix ridge_aux (scalar lambda,
                           const matrix my,
                           const matrix mX)
    # Ridge function using auxiliary OLS
    # Here we do not obtain the Cov(b) matrix,
    # return is just Kx1 beta vector.
    K = cols(mX)
    matrix mXa = mX | (sqrt(lambda) * I(K))
    matrix mya = my | zeros(K,1)
    matrix beta = mols(mya, mXa)
    return beta
end function
