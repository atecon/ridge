


include "/home/at/git/ridge/cv_data_generator.inp"



function bundle default_ridge_opts ()

    # Set default values
    bundle self = null
    scalar self.nlambda = 100		# no. of shrinkage params to check
    scalar self.lmax = 400			# maximal shrinkage param value
    matrix self.lambdas = vec(seq(0.1, self.lmax, self.lmax/self.nlambda))

    # For gridsearch
    # TODO: use also in-sample based information criteria
    string self.loss_type = "rmse"		# Loss function: 'rmse', 'mae', 'mape'"
    if self.loss_type == "rmse"
        scalar self.sel_loss = 2
    elif self.loss_type == "mae"
        scalar self.sel_loss = 3
    elif self.loss_type == "mape"
        scalar self.sel_loss = 5
    endif
    
    # for cv_model_selection package
    string self.cv_type = "kfold"	# Type of cross-validation: "kfold", "loo"
    scalar self.n_folds = 5			# no. of folds for "kfold"

    scalar self.printout = 1		# Print results (default: True)

    return self
end function


function bundle ridge (series y "Dep. variable",
                       const list X "Regressors",
                       bundle opts[null])

    /* Input data is standardized before doing ridge.
       The output is rescaled to match the original data.
       Results for a constant (if present) are ordered first.
       A scalar input for lambdas is treated by gretl as 1x1 matrix.
       Returns a bundle with:
       coeffs: KxL matrix of ridge estimators, L being number of lambdas;
       If a constant term was originally included, its coefficient
       is estimated as mean(y - Xtilde'beta), where Xtilde does not
       contain the constant.
       tstats: KxL matrix of t-statistics; no tstat is available for
       the constant term, so if a constant term was given then
       tstats[1] is NA.
       vcvs: L-element array of matrices, each Kbar x Kbar.
       Kbar is K, or K-1 if a constant term was originally included.
       uhats: TxL matrix of residual vectors, one for each lambda
       s2s: Lx1 vector of residual variance estimates (res'res / T)
       FIXME: R2 ?
    */

    /*=======================*/
    /* Main package function */
    /*=======================*/
    
    # Set up the bundle
    bundle self = default_ridge_opts()
    if exists(opts)
        # override defaults
        self = opts + self
        self.nlambda = rows(vec(self.lambdas))
    endif

    # Drop missings
    smpl --no-missing y X
    
    # Put some stuff to the bundle
    #=============================
    series self.y = y
    list self.X = X
    list self.Xtilde = X - const    
    # check for constant
    scalar self.wconst = nelem(self.Xtilde) < nelem(self.X) ? 1 : 0

    # standardize and save factors for later
    matrix my = cdemean({y})
    scalar self.ysd = sdc(my)
    self.my = my ./ self.ysd
    matrix mX = cdemean({self.Xtilde})
    matrix self.Xsd = sdc(mX)			# gives row vector
    self.mX = mX ./ self.Xsd


    # Obtain test/train datasets for cross-validation
    # We use the 2cv_data_generator" package for this
    #================================================
    bundle bcv = null
    bcv.X = mX
    bcv.y = my
    genr index
    matrix mindex = {index}
    bcv.index = mindex
    # further options for cv
    bcv.cv_type = self.cv_type
    bcv.n_folds = self.n_folds
    bundle bcv = CrossValidation(bcv)
    # store output in self
    self.foldsize = bcv.foldsize
    self.y_test = bcv.y_test
    self.y_train = bcv.y_train
    self.X_test = bcv.X_test
    self.X_train = bcv.X_train


    # Run cross-validation
    #======================
    cv_ridge(&self)		# returns the "cv_loss" 5-dim. array
    # Each matrix in cv_loss is of dim "n_lambdas by n_folds" matrices holding forecast losses

    
    # Determine optimally trained model conditional
    # on some loss metric
    #================================================
    get_optimal_ridge(&self)    
# matrix cv_mean_loss = log(self.lambdas) ~ meanr(self.cv_loss[self.sel_loss][,2:])

    
    

    











    # labels
    strings self.Xnames = varnames(Xreorder)    
    rnameset(self.betas, self.Xnames)
    rnameset(self.tstats, self.Xnames)
    /*
    if self.printout
        strings lbl = self.Xnames + defarray("Penalty", "Resid SD")
        print "Ridge regression results"
        loop i=1..self.nlambda -q
            matrix cmat = self.betas[,i] ~ (self.betas[,i] ./ self.tstats[,i])
            matrix extra = self.lambdas[i] | sqrt(self.s2s[i])
            modprint cmat lbl extra
        endloop
    endif
    */

    # copy to output
    /*
    return defbundle("lambdas", self.lambdas, "coeffs", self.betas, \
      "tstats", self.tstats, "vcvs", self.betaCovs, "uhats", self.resids, \
      "s2s", self.s2s, "Xnames", self.Xnames)      
    */
    return self
end function



function bundle get_optimal_ridge (bundle *self "Created bundle from calling ridge() before")

    /* This function evaluates all computed loss values for a given metric
    and determines the optimal shrinkage parameter 'lamba'. Fits the model
    and returns its point estimate and additional regression information. */


    # Conditional on a selected cross-validation metric, get the optimal 
    # shrinkage parameter.
    #-------------------------------------------------------------------
    matrix cv = self.cv_loss[self.sel_loss]	# lambdas ~ loss_for_each_fold
    # Obtain statitics across folds    
    matrix cv_loss = meanr(cv[,2:]) ~ sdc((cv[,2:])')'

    # Minimize mean(loss)
    scalar rowsel = iminc(cv_loss[,1])
    self.lambda_min = self.lambdas[rowsel]				# 'lambda' minimizing mean(loss)
    self.loss_min = cv_loss[rowsel,1]					# assosciated loss    
    # Minimize sd(loss)
    scalar rowsel = iminc(cv_loss[,2])
    self.lambda_min_se = self.lambdas[rowsel]			# 'lambda' minimizing sd(loss)
    self.loss_min_se = cv_loss[rowsel,2]				# 'lambda' minimizing sd(loss)

    


    #rnameset(FCstat, cnam)
    scalar pos_lambda = iminc(loss)					# position of lambda minimizing loss
    scalar self.lambda_opt = self.lambdas[pos_lambda]	# optimal lambda value
    scalar self.loss_opt = loss[pos_lambda]
    matrix self.loss = loss
    matrix self.beta_opt = self.betas[,pos_lambda]		# optimal coefficient vector
    rnameset(self.beta_opt, self.Xnames)

#    string self.loss_type = loss_type
    if self.printout
        printf "\n*** '%s' minimized for lambda = %.2f ***\n", self.loss_type, self.lambda_opt
        printf "*** Loss ('%s') is = %.5f ***\n", self.loss_type, self.loss_opt
    endif
    
    return self
end function





function matrices gen_cv_loss_mat (bundle *self)
    /* helper function generating an array of 
    matrices for holding various CV eval. metrics */
    scalar n_stats = 5			# ME, RMSE, MAE, MPE, MAPE    
    matrices cv_loss = array(n_stats)
    loop i=1..n_stats -q
        cv_loss[i] = vec(self.lambdas) ~ zeros(self.nlambda, self.n_folds)
    endloop
    return cv_loss    
end function    



function bundle cv_ridge (bundle *self)

    /* Cross-validation for Ridge estimator
    using the cv_data_generator package */
    
    # Generate array of matrices holding various forecast eval. metrics
    matrices cv_loss = gen_fc_stats_mat(&self)    
    strings cnam = array(1+self.n_folds)
    cnam[1] = "Lambda"
    
    loop fold_loop=1..self.n_folds -q
        # string construction
        cnam[1+fold_loop] = sprintf("fold=%d",fold_loop)

        # Set up the training and test set
        matrix my_train = self.y_train[fold_loop][,2:]
        matrix mX_train = self.X_train[fold_loop][,2:]
        matrix my_test = self.y_test[fold_loop][,2:]
        matrix mX_test = self.X_test[fold_loop][,2:]
        if self.wconst	# Add intercept to test set            
            mX_test = ones(rows(mX_test),1) ~ mX_test
        endif
        
        # Loop over shrinkage values
        loop i = 1..self.nlambda -q

            # Train the model
            matrix temp = ridge_svd(self.lambdas[i], my_train, mX_train)

            bhat = temp[,1]
            # rescale
            bhat = bhat .* (sdc(my_train) ./ sdc(mX_train)')

            # construct residuals
            matrix res = my_train - mX_train*bhat 	# original data (w/o const)

            # resids may have non-zero mean: insert the constant term
            if self.wconst
                bhat = meanc(res) | bhat
            endif

            # Fit the model to test set + compute forecast loss
            matrix mat = fcstats(my_test, mX_test*bhat)
            loop j=1..5 -q 							# ME, RMSE, MAE, MPE, MAPE
                cv_loss[j][i,1+fold_loop] = mat[j]
            endloop
        endloop
    endloop
    
    # attache column names
    loop j=1..5 -q
        cnameset(cv_loss[j], cnam)
        self.cv_loss = cv_loss	# array of "n_lambdas by n_folds" matrices holding forecast loss
    endloop
        
    return self
end function



function bundle ridge_gridsearch (bundle *self "Created bundle from calling ridge() before")

    /* This function is supposed to run a grid-search and select the
    optimal lambda-value for some loss function. Most likely you want
    to use for y and X observations from the test set. */

    #smpl --no-missing y X

    # store loss values
    matrix loss = zeros(self.nlambda,1)

    # Compute forecast evaluation statistics for each eta value
    # Predict and evaluate
    loop i=1..self.nlambda -q
        matrix fc = {self.X} * self.betas[,i]		# construct forecast
        loss[i] = fcstats(self.y, fc)[self.sel_loss]
        #cnam[i] = sprintf("lambda=%g", b.lambdas[i])
    endloop
    #rnameset(FCstat, cnam)
    scalar pos_lambda = iminc(loss)					# position of lambda minimizing loss
    scalar self.lambda_opt = self.lambdas[pos_lambda]	# optimal lambda value
    scalar self.loss_opt = loss[pos_lambda]
    matrix self.loss = loss
    matrix self.beta_opt = self.betas[,pos_lambda]		# optimal coefficient vector
    rnameset(self.beta_opt, self.Xnames)

#    string self.loss_type = loss_type
    if self.printout
        printf "\n*** '%s' minimized for lambda = %.2f ***\n", self.loss_type, self.lambda_opt
        printf "*** Loss ('%s') is = %.5f ***\n", self.loss_type, self.loss_opt
    endif
    
    return self
end function


function bundle est_ridge (bundle *self)

    /* Actual computation of ridge and related stats */
    
    matrix res = zeros($nobs,1)
    matrix temp = {}
    
    # Matrix holder
    matrix self.betas = zeros(nelem(self.Xtilde), self.nlambda)
    matrix self.tstats = zeros(nelem(self.Xtilde), self.nlambda)
    matrix self.resids = zeros($nobs, self.nlambda)
    matrix self.s2s = zeros(self.nlambda, 1)
    matrices self.betaCovs = array(self.nlambda)

    loop i = 1..self.nlambda -q
        matrix temp = ridge_svd(self.lambdas[i], self.my, self.mX)
        self.betas[,i] = temp[,1]
        self.betaCovs[i] = temp[, 2:]
        self.tstats[,i] = self.betas[,i] ./ sqrt(diag(temp[, 2:]))
        
        # rescale
        self.betas[,i] = self.betas[,i] .* (self.ysd ./ self.Xsd')
        self.betaCovs[i] = self.betaCovs[i] * self.ysd^2 ./ (self.Xsd'self.Xsd)
        
        # construct residuals
        matrix res = {self.y} - {self.Xtilde} * self.betas[,i] # original data (w/o const)
        self.resids[,i] = res				# resids may have non-zero mean
        
        # estimate residual variance
        res = cdemean(res)
        self.s2s[i] = res'res / rows(self.my)
    endloop

    return self
end function



function matrix ridge_svd (scalar lambda,
                           const matrix my,
                           const matrix mX,
                           matrix *resids[null],
                           bool withcov[1])
    # Ridge function using SVD
    # my: Tx1 vector
    # mX: TxK design matrix
    matrix out, U, Vt	# Vt: V transposed
    matrix sv = svd(mX, &U, &Vt)
    matrix E_lda_row = 1 / (sv.^2 + lambda)
    if withcov	# we will need the ridge inverse later
        # ridge inverse
        matrix ridgeI = (Vt' .* E_lda_row) * Vt
        # beta, using this inverse
        out = ridgeI * mX'my
        matrix res = my - mX*out
        scalar s2 = res'res / rows(mX)
        matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
        out ~= covbeta
    else
        # beta, using the SVD output differently
        out = ( Vt' .* (sv .* E_lda_row) ) * U'my
    endif
    if exists(resids)
        resids = withcov ? res : my - mX*out
    endif
    return out
end function

function matrix ridge_naive (scalar lambda,
                             const matrix my,
                             const matrix mX,
                             matrix *resids[null],
                             bool withcov[1])
    # Ridge function using matrix algebraic formulas directly
    # Returns a K x (1 + K) matrix if withcov==1,
    # the first col is the estimate beta_ridge,
    # followed by the KxK matrix Cov(beta_ridge).
    # Otherwise just a Kx1 matrix.
    # If 'resids' is provided, it will be filled with residuals.
    matrix ridgeI = inv(mX'mX + lambda * I(cols(mX)))
    # beta
    matrix out = ridgeI * mX'my
    if exists(resids) || withcov
        matrix res = my - mX*out # y - Xb
        if withcov
            scalar s2 = res'res / rows(mX)
            matrix covbeta = s2 * ridgeI * mX'mX * ridgeI
            out ~= covbeta
        endif
        if exists(resids)
            resids = res
        endif
    endif
    return out
end function

function matrix ridge_aux (scalar lambda,
                           const matrix my,
                           const matrix mX)
    # Ridge function using auxiliary OLS
    # Here we do not obtain the Cov(b) matrix,
    # return is just Kx1 beta vector.
    K = cols(mX)
    matrix mXa = mX | (sqrt(lambda) * I(K))
    matrix mya = my | zeros(K,1)
    matrix beta = mols(mya, mXa)
    return beta
end function


function void plot_loss (bundle *self,
                         string path "Set path or 'display'")
    # Plot the loss function for each iteration
    # TODO: glmnet also computes CIs based on CV-results
    
    matrix mat = ( vec(self.loss) ~ log(vec(self.lambdas)) )
    scalar ymx = maxc(mat[,1])*1.02
    plot mat
        options with-lines fit=none
        printf "set ylabel '%s' font ', 12'", self.loss_type
        printf "set xlabel 'log(lambda)' font ', 12'"
        literal set linetype 1 lc rgb "red" lw 2
        literal set linetype 2 lc rgb "black" lw 2 pt 3
        printf "set yrange[:%g]", ymx
        printf "set key outside right"
        printf "set arrow from %g,graph(0,0) to %g,graph(1,1) \
        nohead lc rgb 'blue' lw 1.5", log(self.lambda_opt), log(self.lambda_opt)
    end plot --output="@path"
end function


function void plot_beta (bundle *self,
                         string path "Set path or 'display'")
    # Plot the solution path as a function of log(lambda)
    matrix mat = self.betas' ~ log(self.lambdas)
    plot mat
        options with-lines fit=none
        printf "set ylabel 'Coefficients' font ', 12'"
        printf "set xlabel 'log(lambda)' font ', 12'"
        printf "set arrow from %g,graph(0,0) to %g,graph(1,1) \
          nohead lc rgb 'blue' lw 1.5", log(self.lambda_opt), log(self.lambda_opt)
        printf "set key outside below"
        literal set grid
    end plot --output="@path"
end function
